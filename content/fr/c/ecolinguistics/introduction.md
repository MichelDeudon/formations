---
title: Introduction
date: '2021-01-01'
type: book
weight: 20
---

Motiver les √©tudes de langues.

<!--more-->

{{< icon name="clock" pack="fas" >}} 1h20 cours d'introduction

## Pourquoi √©tudier les langues?

{{< youtube -R6YMWb0vUA >}}
<br> 

Kate Jefferey est professeur de neurosciences √† UCL, Londres, et responsable scientifique chez Extinction Rebellion. Dans son discours inspirant sur la [psychologie de l'inaction climatique](https://www.youtube.com/watch?v=-R6YMWb0vUA&ab_channel=UCLMindsLunchHourLectures), le langage, en tant que moyen de communiquer les uns avec les autres et de collaborer, joue un r√¥le cl√© jouer dans <b>la fa√ßon dont nous comprenons le pass√©, envisageons l'avenir et g√©rons le pr√©sent</b>. <i>Avec le langage, nous sommes all√©s plus loin que n'importe quelle esp√®ce sur Terre, nous sommes all√©s sur la Lune</i>. Et si nous commencions tous √† apprendre une langue avec empathie et utilisions le langage pour r√©soudre certains de nos plus gros d√©fis? <br>

Le langage joue un r√¥le fondamental dans la compr√©hension des forces cach√©es qui fa√ßonnent nos d√©cisions. Nous devons embrasser le langage et notre irrationalit√© pour imaginer et co-cr√©er un avenir meilleur.

<b>La linguistique computationnelle</b> est un domaine interdisciplinaire qui traite des langues, de la psychologie, des sciences sociales, des statistiques, de l'informatique, de l'intelligence artificielle, etc. Elle a gagn√© en popularit√© au cours de la derni√®re d√©cennie avec la publication d'ensembles de donn√©es, de biblioth√®ques et de cours en acc√®s libre. Les mod√®les se sont am√©lior√©s sur diff√©rents benchmarks (par exemple, la traduction). Cependant, cette augmentation des performances s'accompagne d'une augmentation drastique de la complexit√© et des ressources n√©cessaires (donn√©es, mat√©riel, √©nergie). Un nouveau paradigme en IA et en linguistique computationnelle est n√©cessaire.

## Pourquoi √©tudier l'innovation frugale?

> <i>Tous les mod√®les sont faux, certains sont utiles.</i>

<b>Les mod√®les en science des donn√©es ont consid√©rablement augment√© en complexit√© au cours des 10 derni√®res ann√©es</b>, √† l'avantage des fournisseurs de cloud comme Google, Microsoft et Amazon üå•Ô∏è. D'abord en vision par ordinateur dans les ann√©es 2012, puis progressivement en linguistique depuis 2014 avec les vecteurs de mots, les plongements de documents et les mod√®les d'attention.

BERT, RoBERT, CamemBERT üßÄ sont des mod√®les avec une complexit√© quadratrique. Les conf√©rences sur l'IA, comme NeurIPS, sont domin√©es par des acteurs qui ex√©cutent ces mod√®les en tant que service. Pourquoi r√©soudre un probl√®me en 5 minutes quand on peut facturer plus pendant des heures ? Ce conflit d'int√©r√™ts peut sembler naif mais c'est ainsi que le domaine est devenu toxique. ü§¢

> <i>L'entra√Ænement d'un seul mod√®le d'IA peut √©mettre autant de carbone que cinq voitures au cours de leur vie (...) Le mod√®le le plus co√ªteux, le BERT, a une empreinte carbone d'environ 1 400 livres d'√©quivalent en dioxyde de carbone, soit pr√®s d'un rond- voyage vol trans-Am√©rique pour une personne.</i> [Technology Review, 2019](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/).

D√©cembre 2022, Elon Musk a publi√© un mod√®le avec 175 millions de param√®tres, 60% de plus que BERT. <b>C'est une question d'ego, qui a le plus grand r√©seau de neurones</b>. BERT √©tait un bazooka. <b>OpenAI a sorti un tank</b>.

Les syst√®mes de prise de d√©cision manquent de <b>diversit√©</b>. Il n'existe pas de mod√®le de langage universel entrain√© par des ing√©nieurs francais sur des donn√©es en anglais. Les conflits d'int√©r√™ts nous √©loignent davantage de nos objectifs communs tels que la construction d'une <b>soci√©t√© inclusive</b> ou d'une <b>√©conomie √† faibles √©missions de carbone</b>.

De plus, la fa√ßon dont l'IA est r√©alis√©e chez Google, Facebook, Microsoft ou Amazon n'est pas appropri√©e pour de nombreux entrepreneurs ou chercheurs, travaillant sur de nouveaux probl√®mes avec peu ou pas de donn√©es. L'exploitation de main-d'≈ìuvre bon march√© et de mod√©rateurs pour superviser les mod√®les d'apprentissage automatique ne sont pas tr√®s √©thiques.

Nous sommes √† la crois√©e des chemins dans la mani√®re dont l'IA, la PNL et la linguistique computationnelle sont enseign√©es. Alors que les grands acteurs continueront √† construire des mod√®les plus complexes, nous nous concentrerons d'abord sur la construction de mod√®les simples, intelligibles et utiles et tenterons de d√©mocratiser l'acc√®s √† la linguistique informatique pour responsabiliser les cr√©ateurs, les entrepreneurs et les chercheurs. Nous poserons les bases scientifiques de la linguistique computationnelle, et n'explorerons pas l'Intelligence G√©n√©rale Artificielle ou les Grands Mod√®les de Langage. En inversant la tendance des grands acteurs, l'innovation frugale peut nous rapprocher de la construction d'une soci√©t√© inclusive et d'une √©conomie bas carbone ü¶ì. Ce cours sur l'innovation frugale et la linguistique informatique est une alternative open source et interdisciplinaire pour les personnes int√©ress√©es √† relever les d√©fis soci√©taux et environnementaux avec les praticiens de l'apprentissage et du d√©veloppement des langues. Le cours explorera diff√©rents cas d'utilisation et mod√®les test√©s pour responsabiliser les cr√©ateurs √† travers des exemples illustr√©s.

## Applications

Voici quelques id√©es sur la fa√ßon dont vous pouvez appliquer ce que vous apprendrez dans ce cours

- Aider les √©tudiants √† apprendre des langues avec des applications ludiques comme [Duolingo](https://www.duolingo.com/).
- Soutenir les ONG de d√©fense des droits de l'Homme en quantifiant et surveillant des [indicateurs de diversit√© et d'inclusion](https://www.mtpcours.fr/c/ecolinguistics/uc1-debias-language/).
- Recommander des articles similaires ou points de vue diff√©rents, par exemple en sant√© ou jurisprudence.
- G√©n√©rer des recettes v√©g√©tariennes de saison, de la musique et de l'art.
- Lutter contre le blanchiment d'argent, l'esclavage moderne, les fausses nouvelles et les discours de haine.

## Quiz

{{< spoiler text="Combien de langues sont parl√©es dans le monde aujourd'hui?" >}}
Plus de 7000 langues sont parl√©es aujourd'hui, mais seulement 23 langues repr√©sentent plus de la moiti√© de la population mondiale. La recherche en science des donn√©es, en PNL et en IA se fait majoritairement en anglais, introduisant un biais dans notre approche de la linguistique computationnelle.
{{< /spoiler >}}

{{< spoiler text="Vrai ou faux, le mod√®le BERT a une empreinte carbone proche d'un vol aller-retour trans-Am√©rique pour une personne?" >}}
Vrai, selon [Technology Review, 2019](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/).
{{< /spoiler >}}

## Reference 

> Emma Strubell, Ganesh Ananya and Andrew McCallum. [Energy and policy considerations for deep learning in NLP](https://arxiv.org/abs/1906.02243). arXiv preprint arXiv:1906.02243 (2019). Published in the 57th Annual Meeting of the Association for Computational Linguistics (ACL). Florence, Italy. July 2019.

> Karen Hao. [Training a single AI model can emit as much carbon as five cars in their lifetimes](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/). MIT Technology Review. June 6, 2019.