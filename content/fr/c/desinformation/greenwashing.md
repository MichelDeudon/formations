---
title: Meta LLaMA et d√©sinformation
date: '2023-02-24'
type: book
weight: 40
tags:
  - √©nergie
  - climat
  - d√©sinformation
---

F√©v 2023. L'entra√Ænement de mod√®les fondamentaux plus petits comme LLaMA est souhaitable (...) car √ßa n√©cessite beaucoup moins de puissance de calcul et de ressources pour tester de nouvelles approches. FAIR Paris. Le blog de Meta AI est une {{<hl>}}perle de greenwashing{{</hl>}}.

<!--more-->

## [Pr√©sentation de LLaMA : un mod√®le de langage fondamental de 65 milliards de param√®tres](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

Dans le cadre de l'engagement de Meta en faveur de la science ouverte, nous publions aujourd'hui LLaMA (Large Language Model Meta AI), un mod√®le de langage fondamental √† l‚Äô√©tat de l‚Äôart con√ßu pour aider les chercheurs √† faire progresser leurs travaux dans ce sous-domaine de l'IA. {{<hl>}}Des mod√®les <b>plus petits</b> et plus performants tels que LLaMA{{</hl>}} permettent √† d'autres membres de la communaut√© de recherche qui n'ont pas acc√®s √† de grandes quantit√©s d'infrastructures d'√©tudier ces mod√®les, d√©mocratisant davantage l'acc√®s dans ce domaine important et en √©volution rapide.

{{<hl>}}L'entra√Ænement de mod√®les fondamentaux <b>plus petits</b> comme LLaMA est souhaitable{{</hl>}} dans le champ des large language model, car {{<hl>}}√ßa n√©cessite beaucoup <b>moins de puissance de calcul et de ressources</b>{{</hl>}} pour tester de nouvelles approches, valider le travail des autres et explorer de nouveaux cas d'utilisation. Les mod√®les fondamentaux sont entra√Æn√©s sur un vaste ensemble de donn√©es non √©tiquet√©es, ce qui les rend id√©aux pour √™tre ajust√© √† une vari√©t√© de t√¢ches. Nous rendons LLaMA disponible en plusieurs tailles (param√®tres 7B, 13B, 33B et 65B) et partageons √©galement une carte du mod√®le LLaMA qui d√©taille comment nous avons construit le mod√®le conform√©ment √† notre approche des {{<hl>}}<b>pratiques d'IA responsable</b>{{</hl>}}.

{{< figure src="meta-paper-hours-gpu.png" caption="Empreinte carbone de diff√©rents mod√®les entrain√©s dans le m√™me centre de donn√©es, tir√© de l'article scientifique LLaMA sur [Arxiv](https://arxiv.org/abs/2302.13971) (voir section 6 Empreinte carbone)." numbered="true">}}

{{< callout warning >}}
<i>L'entra√Ænement de nos mod√®les a consomm√© une quantit√© massive d'√©nergie, responsable de l'√©mission de dioxyde de carbone.</i> (voir section 6 Empreinte carbone). <i>Nous pr√©voyons de publier √† l'avenir des mod√®les plus grands, entrain√©s sur des corpus plus importants</i> (voir section 8 Conclusion). [Arxiv](https://arxiv.org/abs/2302.13971).
{{< /callout >}}

{{< callout note >}}
Aucune communication n'est faite sur [le blog](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) de Meta AI Research, sur les [r√©seau sociaux](https://www.linkedin.com/posts/yann-lecun_github-facebookresearchllama-inference-activity-7034956639526952960-B1-d?trk=public_profile_like_view) ou sur [Github](https://github.com/facebookresearch/llama/blob/1076b9c51c77ad06e9d7ba8a4c6df775741732 ) concernant l'empreinte carbone record, l'impact environnemental et le financement du mod√®le LLaMA version 1.
{{< /callout >}}

{{<hl>}}Des mod√®les <b>plus petits</b>{{</hl>}} entra√Æn√©s sur plus de jetons - qui sont des morceaux de mots - sont {{<hl>}}<b>plus faciles</b> √† r√©entra√Æner et affiner{{</hl>}} pour des cas d'utilisation potentiels et sp√©cifiques de produits. Nous avons entra√Æn√© LLaMA 65B et LLaMA 33B sur 1,4 trilliard de jetons. Notre {{<hl>}}<b>plus petit</b> mod√®le{{</hl>}}, LLaMA 7B, est form√© sur un trilliard de jetons‚Ä¶

{{< callout note >}}
En r√©alit√© on change d‚Äô√©chelle. Millions, milliards, trilliards... C‚Äôest les mod√®les de 2017-2019 x1000 en complexit√© en 4 ans.
{{< /callout >}}

Il reste encore des recherches √† faire pour traiter les risques de biais, de commentaires toxiques et d'hallucinations dans les grands mod√®les de langage. Comme d'autres mod√®les, LLaMA partage ces d√©fis... Nous fournissons √©galement dans l'article un ensemble d'√©valuations sur les biais et la toxicit√© du mod√®le pour montrer les limites du mod√®le et pour soutenir la poursuite des recherches dans ce domaine crucial.

{{<hl>}}Pour maintenir l'<b>int√©grit√©</b> et pr√©venir les <b>abus</b>{{</hl>}}, nous publions notre mod√®le sous une licence non commerciale ax√©e sur les cas d'utilisation de la recherche. L'acc√®s au mod√®le sera accord√© au cas par cas aux chercheurs universitaires; ceux affili√©s √† des organisations du gouvernement, de la soci√©t√© civile et du milieu universitaire; et les laboratoires de recherche de l'industrie √† travers le monde...

{{< callout warning >}}
08 mars 2023. [Fuite du mod√®le](https://www.01net.com/actualites/fuite-meta-alternative-chatgpt-meta-partagee-forum.html). Int√©grit√© et pr√©vention des abus? Fausses nouvelles et mod√®le √©conomique?
{{< /callout >}}

## [Consid√©rations √©thiques sur Github](https://github.com/facebookresearch/llama/blob/1076b9c51c77ad06e9d7ba8a4c6df775741732bd/MODEL_CARD.md)
- Organisation d√©veloppant le mod√®le: L'√©quipe FAIR de Meta AI.
- Date du mod√®le: LLaMA a √©t√© entra√Æn√© entre d√©cembre 2022 et f√©vrier 2023.
- Version du mod√®le: Il s'agit de la version 1 du mod√®le.
- Licence: Licence sur mesure non commerciale
- Donn√©es: Les donn√©es utilis√©es pour entra√Æner le mod√®le sont collect√©es √† partir de diverses sources, principalement du Web. En tant que tel, il contient un contenu offensant, pr√©judiciable et biais√©. Nous nous attendons √† ce que le mod√®le pr√©sente de tels biais.
- Vie humaine: Le mod√®le n'est pas destin√© √† √©clairer les d√©cisions sur les questions essentielles √† la vie humaine et ne doit pas √™tre utilis√© de cette mani√®re.
- Att√©nuations: Nous avons filtr√© les donn√©es du Web en fonction de leur proximit√© avec des textes et r√©f√©rences de Wikip√©dia.
- Risques et pr√©judices: Les risques et pr√©judices des grands mod√®les linguistiques incluent la g√©n√©ration de contenu nuisible, offensant ou biais√©. Ces mod√®les sont souvent susceptibles de g√©n√©rer des informations incorrectes. Nous ne nous attendons pas √† ce que notre mod√®le soit une exception √† cet √©gard.
- Cas d'utilisation: LLaMA est un mod√®le fondamental et, en tant que tel, il ne doit pas √™tre utilis√© pour des applications sans une enqu√™te plus approfondie et une att√©nuation des risques. Ces risques et cas d'utilisation potentiellement dangereux incluent, mais sans s'y limiter : la <b>g√©n√©ration de fausses informations</b> et la <b>g√©n√©ration de contenu nuisible, biais√© ou offensant</b>.

{{< figure src="meta-github-bias.png" caption="Meta github model bias" numbered="true">}}

## Ressources pour lutter contre le greenwashing
- Le guide anti greenwashing de [Pour un R√©veil Ecologique](https://pour-un-reveil-ecologique.org/fr/les-entreprises-nous-repondent/#guide-anti-greenwashing)
- L'outil en ligne anti greenwashing de l'[ADEME](https://communication-responsable.ademe.fr/antigreenwashing)

## Hiver de l'IA et du Climat (2023)

### üî• Records de consommation √©nerg√©tique et d√©sinformation
- 14-03 (2023) <b style="color:blue;">Open AI</b> ChatGPT4. <i>Explosion de la d√©sinformation</i>. [BFM Business](https://www.bfmtv.com/tech/intelligence-artificielle/le-patron-de-l-entreprise-a-l-origine-de-chat-gpt-a-un-peu-peur-de-chat-gpt_AV-202303210270.html)
- 24-02 (2023) <b style="color:blue;">Facebook</b> AI Research LLaMA 65B.
- 30-11 (2022) <b style="color:blue;">Open AI</b> ChatGPT3. 
- 25-11 (2022) <b style="color:blue;">Facebook</b> AI Research Galactica 120B. <i>Plus √† venir</i>. @[Paperswithcode](https://paperswithcode.com/paper/galactica-a-large-language-model-for-science-1) @[HuggingFace](https://huggingface.co/facebook/galactica-120b).
- 14-02 (2019) üî• <b style="color:blue;">OpenAI</b> ChatGPT2.
- 14-02 (2019) <i>Une IA qui √©crit une prose convaincante risque de produire en masse de fausses nouvelles</i>. [MIT Technology Review](https://www.technologyreview.com/2019/02/14/137426/an-ai-tool-auto-generates-fake-news-bogus-tweets-and-plenty-of-gibberish/).

### üíß Crise climatique
- 22-02 <b style="color:red;">Meteo France</b>. <i>S√©cheresse: 32 jours sans pluie en France, record battu</i>. [meteofrance.com](https://meteofrance.com/actualites-et-dossiers/actualites/climat/secheresse-32-jours-sans-pluie-en-france-record-battu).
- 15-02 <b style="color:red;">CNRS</b>. Climatosceptiques: sur Twitter, enqu√™te sur les mercenaires de l‚Äôintox [cnrs.fr](https://lejournal.cnrs.fr/articles/climatosceptiques-sur-twitter-enquete-sur-les-mercenaires-de-lintox) [Le Monde](https://www.lemonde.fr/planete/article/2023/02/13/la-france-fait-face-a-un-fort-regain-de-climatoscepticisme-sur-twitter_6161691_3244.html) @[Audrey Garric](https://twitter.com/audreygarric/status/1625416947729944579?cxt=HHwWhsC-1cSG0o4tAAAA).
- 23-01 <b style="color:red;">Meteo France</b>. <i>2022, ann√©e la plus chaude en France</i>. [meteofrance.com](https://meteofrance.com/actualites-et-dossiers/actualites/2022-annee-la-plus-chaude-en-france).
- 31-12 (2022) <b style="color:blue;">Emmanuel Macron</b> <i>Qui aurait pu pr√©dire la crise climatique?</i> [archive INA](https://www.youtube.com/watch?v=SsqYCvJvxQY&ab_channel=INAPolitique).

{{< vimeo 767800360 >}}