---
title: Consid√©rations √©nerg√©tiques et politiques
date: '2019-06-05'
type: book
weight: 20
highlight: true
tags:
  - intelligence artificielle
  - √©nergie
  - climat
  - d√©sinformation
---

Printemps 2019. Alerte d'Emma Strubell et du MIT.

<!--more-->

## [Consid√©rations √©nerg√©tiques et politiques pour l'IA](https://arxiv.org/abs/1906.02243)

üö® Le 5 juin 2019, Emma Strubell, candidate au doctorat et auteur principal de l'article [Energy and Policy Considerations for Deep Learning in NLP](https://arxiv.org/abs/1906.02243) donnait l'alerte √† la communaut√© scientifique et politique du terrible impact √©cologique des mod√®les de deep learning en linguistique, √† l'√©poque. Son article est paru dans <b>ACL2019</b> üáÆüáπ, la plus grande conf√©rence de linguistique. Le <b>Massachusetts Institute of Technology</b> a rediffus√© l'alerte sur la consommation √©nerg√©tique, les √©missions carbones de mod√®les avec des millions de param√®tres comme Transformers (papier "[Attention is all you need](https://arxiv.org/abs/1706.03762)" de <b style="color:blue;">Google</b>, 2017) et [BERT](https://arxiv.org/abs/1810.04805) (<b style="color:blue;">Google</b>, 2018) dans sa revue technologique du 6 juin 2019. On parlait alors de mod√®les allant jusqu'√† 65 millions de param√®tres (1/1000 [LLaMA](https://arxiv.org/abs/2302.13971), 2023).

üî• Le lendemain de l'alerte d'[Emma Strubell](https://arxiv.org/abs/1906.02243) et du [MIT](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/), le 7 juin 2019, BERT recoit le prix du meilleur article long √† [NAACL19](https://aclanthology.org/N19-1423/) üá∫üá∏. Trois jours plus tard, le 10 juin 2019, 22 auteurs, dont le CEO de <b style="color:blue;">Deep Mind</b> Demis Hassabis, publient sur [Arxiv](https://arxiv.org/abs/1906.05433) un article intitul√© "Lutter contre le changement climatique avec l'IA", republi√© sur [ACM](https://dl.acm.org/doi/10.1145/3485128) d√©but f√©vrier 2022.

## [L'entra√Ænement d'un seul mod√®le d'IA peut √©mettre autant de carbone que...](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/)

6 juin 2019. [MIT Technology Review](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/). Par Karen Hao.

{{< figure src="2019-06 MIT.jpg" caption="MIT Technology Review, juin 2019." numbered="true">}}

L'industrie de l'[intelligence artificielle](https://www.technologyreview.com/artificial-intelligence/) est souvent compar√©e √† l'industrie p√©troli√®re : une fois extraites et raffin√©es, les donn√©es, comme le p√©trole, peuvent √™tre une marchandise tr√®s lucrative. Maintenant, il semble que la m√©taphore puisse s'√©tendre encore plus loin. {{<hl>}}Comme son homologue fossile, <b>le processus de deep learning a un impact environnemental d√©mesur√©</b>{{</hl>}}.

Dans un [nouvel article](http://arxiv.org/abs/1906.02243), des chercheurs de l'Universit√© du Massachusetts √† Amherst ont effectu√© une √©valuation du cycle de vie pour entra√Æner plusieurs grands mod√®les d'IA courants. Ils ont d√©couvert que le processus peut √©mettre plus de 280 tonnes d'√©quivalent en dioxyde de carbone, soit <b>pr√®s de cinq fois les √©missions li√©es au cycle de vie d'une voiture am√©ricaine moyenne</b> (et cela inclut la fabrication de la voiture elle-m√™me). De plus, les chercheurs notent que les chiffres ne doivent √™tre consid√©r√©s que comme des valeurs de r√©f√©rence. "<b>Entrainer un seul mod√®le est le minimum de travail que vous pouvez faire</b>", d√©clare <b>Emma Strubell</b>, candidate au doctorat √† l'Universit√© du Massachusetts, Amherst, et auteur principal de l'article. En pratique, les chercheurs en IA d√©veloppent un nouveau mod√®le √† partir de z√©ro ou adaptent un mod√®le √† un nouveau jeu de donn√©es, n√©cessitant de nombreuses autres exp√©riences d'entra√Ænements et d'ajustements.

### L'empreinte carbone du traitement du langage naturel
L'article examine sp√©cifiquement le processus d'entra√Ænement de mod√®les de linguistique pour le [traitement du langage naturel](https://www.technologyreview.com/s/612975/ai-natural-language-processing-explained/) (NLP), le sous-domaine de l'IA qui se concentre sur la compr√©hension du langage humain par des machines. Les deux derni√®res ann√©es ont √©t√© jalonn√©es de performances remarquables dans la traduction automatique, la compl√©tion de phrases et d'autres t√¢ches. Le [tristement c√©l√®bre mod√®le GPT-2 d'OpenAI](https://www.technologyreview.com/s/612960/an-ai-tool-auto-generates-fake-news-bogus-tweets-and-plenty-of-gibberish/), par exemple, excellait dans la r√©daction de faux articles convaincants. Mais de telles avanc√©es ont n√©cessit√© <b>l'entra√Ænement de mod√®les toujours plus grands sur des ensembles de donn√©es tentaculaires</b> r√©cup√©r√©es sur Internet. <b>L'approche est co√ªteuse en calcul et tr√®s gourmande en √©nergie</b>.

L'√©quipe a examin√© quatre mod√®les qui ont √©t√© √† l'origine des plus grandes avanc√©es en mati√®re de performances : le Transformer, ELMo, [BERT](https://www.nytimes.com/2018/11/18/technology/artificial-intelligence-language.html) et GPT-2. Ils les ont entra√Æn√©s chacun sur un seul GPU pendant jusqu'√† une journ√©e pour mesurer sa consommation d'√©nergie. Ils ont ensuite utilis√© le nombre d'heures d'entra√Ænement indiqu√© dans les documents originaux du mod√®le pour calculer l'√©nergie totale consomm√©e au cours du processus d'entra√Ænement complet. Ce nombre a √©t√© converti en livres d'√©quivalent dioxyde de carbone sur la base du mix √©nerg√©tique moyen aux √âtats-Unis, qui correspond √©troitement au mix √©nerg√©tique utilis√© par <b style="color:blue;">AWS</b> d'Amazon, le plus grand fournisseur de services cloud.

{{< callout note >}}
F√©vrier 2023 <b>Macron</b> d√©core <b style="color:blue;">Bezos</b> en secret. [Le Point](https://www.youtube.com/watch?v=kZPG9rmbdmw&ab_channel=LePoint).
{{< /callout >}}

### Les co√ªts estim√©s d'entra√Æner un mod√®le une fois
En pratique, les mod√®les sont g√©n√©ralement entra√Æn√©s plusieurs fois au cours de la recherche et du d√©veloppement. Ils ont constat√© que <b>les co√ªts informatiques et environnementaux de l'entra√Ænement augmentent proportionnellement √† la taille du mod√®le, puis explosent lorsque des √©tapes de r√©glage suppl√©mentaires sont utilis√©es pour augmenter le score final du mod√®le.</b>

Strubell et ses coll√®gues ont utilis√© un mod√®le qu'ils avaient produit dans un article pr√©c√©dent comme √©tude de cas. Ils ont constat√© que le processus de construction et de test d'un mod√®le final digne d'un article n√©cessite l'entra√Ænement de 4 789 mod√®les sur une p√©riode de six mois. Converti en √©quivalent CO2, il a √©mis plus de 35 tonnes.

L'importance de ces chiffres est colossale, surtout si l'on consid√®re les tendances actuelles de la recherche en IA. "<i>Ce type d'analyse doit √™tre fait pour sensibiliser sur les ressources d√©pens√©es [...] et susciter un d√©bat</i>", explique G√≥mez-Rodr√≠guez. "<i>Ce que beaucoup d'entre nous n'ont probablement pas compris, c'est son ampleur jusqu'√† ce que nous ayons vu ces comparaisons</i>", a fait √©cho Siva Reddy, post-doctorant √† l'<b>Universit√© de Stanford</b> qui n'a pas particip√© √† la recherche.

### La privatisation de la recherche en IA
Les r√©sultats soulignent √©galement un autre probl√®me croissant dans le domaine de l'IA : <b>l'intensit√© des ressources d√©sormais n√©cessaires pour produire des r√©sultats dignes d'√™tre publi√©s</b> rend de plus en plus difficile pour les personnes travaillant dans le milieu universitaire de continuer √† contribuer √† la recherche. "<b>Cette tendance √† former d'√©normes mod√®les sur des tonnes de donn√©es n'est pas r√©alisable pour les universitaires</b>, en particulier les √©tudiants dipl√¥m√©s, car nous n'avons pas les <b>ressources de calcul</b>", d√©clare Strubell. "Il y a donc un <b>probl√®me d'acc√®s √©quitable</b> entre les chercheurs du milieu universitaire et les chercheurs de l'industrie."

{{< figure src="macron-lecun-zuckerberg.png" caption="Macron Lecun Zuckerberg et Vivatech 2023. <b style='color:blue;'>Zuckerberg</b>-<b>Macron</b> fait le buzz √† VivaTech. [Les echos](https://www.lesechos.fr/start-up/next40-vivatech/le-duo-zuckerberg-macron-fait-le-buzz-a-vivatech-132831), Mai 2018. <b style='color:blue;'>Zuckerberg</b> et <b>Macron</b> se sont (encore) rencontr√©s √† l'√âlys√©e. <i>Cinq jours avant la deuxi√®me √©dition 'Tech for good'</i>. [Huffington post](https://www.huffingtonpost.fr/politique/article/mark-zuckerberg-et-emmanuel-macron-se-sont-encore-rencontres-a-l-elysee_144827.html), Mai 2019.">}}

{{< figure src="mafia-these-CIFRE.png">}}

{{< figure src="mafia-france-ia.png" caption="Source: @[Yann Lecun](https://twitter.com/ylecun/status/1629845738170597376?lang=en).">}}

## L'affaire du Si√®cle ‚öñ (2021)
- 24-04 <i>Le bluff num√©rique, ce sont tous les impacts qu‚Äôon ne voit pas</i>, avec <b>Laurie Marrauld</b> du <b>Shift Project</b> et <b>C√©dric Villani</b>. [Lib√©ration](https://www.youtube.com/watch?v=6kJYR0oG3GQ&ab_channel=Lib%C3%A9ration).
- 22-08 <b>Convention citoyenne pour le climat</b>. Une partie des 146 propositions est retenue par le chef de l'√âtat. Code de l'environnement. [LOI n¬∞ 2021-1104 du 22 ao√ªt 2021 portant lutte contre le d√©r√®glement climatique et renforcement de la r√©silience face √† ses effets (V)](https://www.legifrance.gouv.fr/jorf/id/JORFTEXT000043956924). [Article L231-1](https://www.legifrance.gouv.fr/codes/article_lc/LEGIARTI000043961211)
- 14-10 <b>Tribunal de Paris</b>. France condamn√©e pour inaction climatique. L'√âtat a l‚Äôobligation de respecter sa trajectoire de r√©duction d‚Äô√©missions de gaz √† effet de serre et doit r√©parer tout d√©passement, d‚Äôici le <b>31 d√©cembre 2022</b>. Chaque sortie de route sur la trajectoire climatique constitue √† pr√©sent une faute qui DOIT √™tre r√©par√©e. [vie-publique.fr](https://www.vie-publique.fr/en-bref/282012-changement-climatique-la-france-condamnee-pour-prejudice-ecologique).

### [Macron appelle √† la <i>sobri√©t√© individuelle</i>](https://www.ladepeche.fr/2022/09/05/direct-crise-de-lenergie-quelles-mesures-complementaires-pourraient-etre-prises-suivez-en-direct-la-conference-demmanuel-macron-10524445.php)

{{< icon name="calendar" pack="fas" >}} Conf√©rence de presse √† l'Elysee, le 5 septembre 2022

{{< callout note >}}
Macron a √©t√© r√©√©lu Pr√©sident le 24 avril 2022, deux mois apr√®s l'invasion de l'Ukraine et d√©but de la crise √©nerg√©tique ‚ö°. [Elys√©e](https://www.elysee.fr/emmanuel-macron).
{{< /callout >}}

<i>"Chacun a son r√¥le √† jouer"</i>, annonce le pr√©sident fran√ßais, appelant √† la sobri√©t√© √©nerg√©tique et estimant que <i>"la meilleure √©nergie est celle qu‚Äôon ne consomme pas"</i>. L‚Äôobjectif √©nonc√© par <b>Emmanuel Macron</b> est <i>"d‚Äô√©conomiser 10 % de ce que nous consommons actuellement"</i>. [Elys√©e](https://www.youtube.com/watch?v=XjC1NqzyGkc&ab_channel=%C3%89lys%C3%A9e).