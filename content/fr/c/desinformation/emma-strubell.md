---
title: Consid√©rations √©nerg√©tiques et politiques
date: '2019-06-05'
type: book
weight: 20
highlight: true
tags:
  - intelligence artificielle
  - √©nergie
  - climat
  - d√©sinformation
  - environnement et soci√©t√©
---

Printemps 2019. Alerte d'Emma Strubell et du MIT.

<!--more-->

## [Consid√©rations √©nerg√©tiques et politiques pour l'IA](https://arxiv.org/abs/1906.02243)

üö® Le 5 juin 2019, Emma Strubell, candidate au doctorat et auteur principal de l'article [Energy and Policy Considerations for Deep Learning in NLP](https://arxiv.org/abs/1906.02243) donnait l'alerte √† la communaut√© scientifique et politique du terrible impact √©cologique des mod√®les de deep learning en linguistique, √† l'√©poque. Son article est paru dans <b>ACL2019</b> üáÆüáπ, la plus grande conf√©rence de linguistique. Le <b>Massachusetts Institute of Technology</b> a rediffus√© l'alerte sur la consommation √©nerg√©tique, les √©missions carbones de mod√®les avec des millions de param√®tres comme [Transformers](https://arxiv.org/abs/1706.03762) (<b>Google</b>, 2017) et [BERT](https://arxiv.org/abs/1810.04805) (<b>Google</b>, 2018) dans sa revue technologique du 6 juin 2019. On parlait alors de mod√®les allant jusqu'√† 65 millions de param√®tres (1/1000 [LLaMA](https://arxiv.org/abs/2302.13971), 2023).

{{< callout warning >}}
Le lendemain de l'alerte d'[Emma Strubell](https://arxiv.org/abs/1906.02243) et du [MIT](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/), BERT recoit le prix du meilleur article long √† [NAACL19](https://aclanthology.org/N19-1423/) üá∫üá∏.
{{< /callout >}}

## [L'entra√Ænement d'un seul mod√®le d'IA peut √©mettre autant de carbone que...](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/)

6 juin 2019. [MIT Technology Review](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/). Par Karen Hao.

{{< figure src="desinfo/2019-06 MIT.jpg" caption="MIT Technology Review, juin 2019." numbered="true">}}

L'industrie de l'[intelligence artificielle](https://www.technologyreview.com/artificial-intelligence/) est souvent compar√©e √† l'industrie p√©troli√®re : une fois extraites et raffin√©es, les donn√©es, comme le p√©trole, peuvent √™tre une marchandise tr√®s lucrative. Maintenant, il semble que la m√©taphore puisse s'√©tendre encore plus loin. {{<hl>}}Comme son homologue fossile, <b>le processus de deep learning a un impact environnemental d√©mesur√©</b>{{</hl>}}.

Dans un [nouvel article](http://arxiv.org/abs/1906.02243), des chercheurs de l'Universit√© du Massachusetts √† Amherst ont effectu√© une √©valuation du cycle de vie pour entra√Æner plusieurs grands mod√®les d'IA courants. Ils ont d√©couvert que le processus peut √©mettre plus de 280 tonnes d'√©quivalent en dioxyde de carbone, soit <b>pr√®s de cinq fois les √©missions li√©es au cycle de vie d'une voiture am√©ricaine moyenne</b> (et cela inclut la fabrication de la voiture elle-m√™me). De plus, les chercheurs notent que les chiffres ne doivent √™tre consid√©r√©s que comme des valeurs de r√©f√©rence. "<b>Entrainer un seul mod√®le est le minimum de travail que vous pouvez faire</b>", d√©clare <b>Emma Strubell</b>, candidate au doctorat √† l'Universit√© du Massachusetts, Amherst, et auteur principal de l'article. En pratique, les chercheurs en IA d√©veloppent un nouveau mod√®le √† partir de z√©ro ou adaptent un mod√®le √† un nouveau jeu de donn√©es, n√©cessitant de nombreuses autres exp√©riences d'entra√Ænements et d'ajustements.

### L'empreinte carbone du traitement du langage naturel
L'article examine sp√©cifiquement le processus d'entra√Ænement de mod√®les de linguistique pour le [traitement du langage naturel](https://www.technologyreview.com/s/612975/ai-natural-language-processing-explained/) (NLP), le sous-domaine de l'IA qui se concentre sur la compr√©hension du langage humain par des machines. Les deux derni√®res ann√©es ont √©t√© jalonn√©es de performances remarquables dans la traduction automatique, la compl√©tion de phrases et d'autres t√¢ches. Le [tristement c√©l√®bre mod√®le GPT-2 d'OpenAI](https://www.technologyreview.com/s/612960/an-ai-tool-auto-generates-fake-news-bogus-tweets-and-plenty-of-gibberish/), par exemple, excellait dans la r√©daction de faux articles convaincants. Mais de telles avanc√©es ont n√©cessit√© <b>l'entra√Ænement de mod√®les toujours plus grands sur des ensembles de donn√©es tentaculaires</b> r√©cup√©r√©es sur Internet. <b>L'approche est co√ªteuse en calcul et tr√®s gourmande en √©nergie</b>.

{{< figure src="desinfo/mit-review-2019 - emissions.png">}}

L'√©quipe a examin√© quatre mod√®les qui ont √©t√© √† l'origine des plus grandes avanc√©es en mati√®re de performances : le Transformer, ELMo, [BERT](https://www.nytimes.com/2018/11/18/technology/artificial-intelligence-language.html) et GPT-2. Ils les ont entra√Æn√©s chacun sur un seul GPU pendant jusqu'√† une journ√©e pour mesurer sa consommation d'√©nergie. Ils ont ensuite utilis√© le nombre d'heures d'entra√Ænement indiqu√© dans les documents originaux du mod√®le pour calculer l'√©nergie totale consomm√©e au cours du processus d'entra√Ænement complet. Ce nombre a √©t√© converti en livres d'√©quivalent dioxyde de carbone sur la base du mix √©nerg√©tique moyen aux √âtats-Unis, qui correspond √©troitement au mix √©nerg√©tique utilis√© par <b>AWS d'Amazon</b>, le plus grand fournisseur de services cloud.

{{< callout warning >}}
F√©vrier 2023 <b>Macron</b> d√©core <b>Bezos</b> en secret. [Le Point](https://www.youtube.com/watch?v=kZPG9rmbdmw&ab_channel=LePoint).
{{< /callout >}}

### Les co√ªts estim√©s d'entra√Æner un mod√®le une fois
En pratique, les mod√®les sont g√©n√©ralement entra√Æn√©s plusieurs fois au cours de la recherche et du d√©veloppement. Ils ont constat√© que <b>les co√ªts informatiques et environnementaux de l'entra√Ænement augmentent proportionnellement √† la taille du mod√®le, puis explosent lorsque des √©tapes de r√©glage suppl√©mentaires sont utilis√©es pour augmenter le score final du mod√®le.</b>

Strubell et ses coll√®gues ont utilis√© un mod√®le qu'ils avaient produit dans un article pr√©c√©dent comme √©tude de cas. Ils ont constat√© que le processus de construction et de test d'un mod√®le final digne d'un article n√©cessite l'entra√Ænement de 4 789 mod√®les sur une p√©riode de six mois. Converti en √©quivalent CO2, il a √©mis plus de 35 tonnes.

L'importance de ces chiffres est colossale, surtout si l'on consid√®re les tendances actuelles de la recherche en IA. "<i>Ce type d'analyse doit √™tre fait pour sensibiliser sur les ressources d√©pens√©es [...] et susciter un d√©bat</i>", explique G√≥mez-Rodr√≠guez. "<i>Ce que beaucoup d'entre nous n'ont probablement pas compris, c'est son ampleur jusqu'√† ce que nous ayons vu ces comparaisons</i>", a fait √©cho Siva Reddy, post-doctorant √† l'<b>Universit√© de Stanford</b> qui n'a pas particip√© √† la recherche.

{{< figure src="desinfo/mit-review-2019-costs.png">}}

### La privatisation de la recherche en IA
Les r√©sultats soulignent √©galement un autre probl√®me croissant dans le domaine de l'IA : <b>l'intensit√© des ressources d√©sormais n√©cessaires pour produire des r√©sultats dignes d'√™tre publi√©s</b> rend de plus en plus difficile pour les personnes travaillant dans le milieu universitaire de continuer √† contribuer √† la recherche. "<b>Cette tendance √† former d'√©normes mod√®les sur des tonnes de donn√©es n'est pas r√©alisable pour les universitaires</b>, en particulier les √©tudiants dipl√¥m√©s, car nous n'avons pas les <b>ressources de calcul</b>", d√©clare Strubell. "Il y a donc un <b>probl√®me d'acc√®s √©quitable</b> entre les chercheurs du milieu universitaire et les chercheurs de l'industrie."

{{< figure src="desinfo/macron-lecun-zuckerberg.png" caption="Le government de Macron a privatis√© la recherche en IA. <b>Zuckerberg</b>-<b>Macron</b> fait le buzz √† VivaTech. [Les echos](https://www.lesechos.fr/start-up/next40-vivatech/le-duo-zuckerberg-macron-fait-le-buzz-a-vivatech-132831), Mai 2018. <b>Zuckerberg</b>-<b>Macron</b> se sont (encore) rencontr√©s √† l'√âlys√©e. <i>Cinq jours avant la deuxi√®me √©dition 'Tech for good'</i>. [Huffington post](https://www.huffingtonpost.fr/politique/article/mark-zuckerberg-et-emmanuel-macron-se-sont-encore-rencontres-a-l-elysee_144827.html), Mai 2019.">}}

{{< callout warning >}}
Avril 2021. <i>Le bluff num√©rique, ce sont tous les impacts qu‚Äôon ne voit pas</i>, avec <b>Laurie Marrauld</b> du <b>Shift Project</b> et <b>C√©dric Villani</b>. [Lib√©ration](https://www.youtube.com/watch?v=6kJYR0oG3GQ&ab_channel=Lib%C3%A9ration).
{{< /callout >}}

{{< figure src="desinfo/mafia-these-CIFRE.png">}}

{{< figure src="desinfo/mafia-france-ia.png" caption="@[Yann Lecun](https://twitter.com/ylecun/status/1629845738170597376?lang=en) en r√©ponse √† la mafia fran√ßaise de l'IA. F√©vrier 2023.">}}

{{< callout warning >}}
<i>L'entra√Ænement de nos mod√®les a consomm√© une quantit√© massive d'√©nergie, responsable de l'√©mission de dioxyde de carbone.</i> (section 6 Empreinte carbone). <i>Nous pr√©voyons de publier √† l'avenir des mod√®les plus grands, entrain√©s sur des corpus plus importants</i> (Conclusion). [Arxiv](https://arxiv.org/abs/2302.13971).
{{< /callout >}}

{{< figure src="desinfo/meta-paper-hours-gpu.png" caption="Empreinte carbone pour entrainer un Meta LLaMA une fois. [Arxiv](https://arxiv.org/abs/2302.13971).">}}