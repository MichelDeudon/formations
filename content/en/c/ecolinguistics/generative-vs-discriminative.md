---
title: Generative vs discriminative learning
date: '2021-01-01'
type: book
weight: 70
---

Create to understand.

<!--more-->

{{< icon name="clock" pack="fas" >}} 1h20 introductory course

> <i> What I cannot create, I do not understand </i> - Richard Feynman.

## From Naive Bayes

xyz

## To latent variables and models

Intro PGM

## Gaussian Mixture Models (GMM)

xyz

**Theory:** Probability review. Bayesian learning.

## Hidden Markov Models (HMM)

{{< youtube jY2E6ExLxaw>}}

## Latent Dirichlet Allocation (LDA)

xyz

**Usecase**: Melody harmonisation. Ableton. Music.

## Variational Auto Encoders (VAE)

VAE can be used to disentangle representations (style vs semantics), and consequently measure similarity between pairs, such as questions.

{{< figure src="linguistics/img7.jpg" caption="Learning to repeat, reformulate. Visualisation of learned representations after dimensionality reduction with PCA.">}}

**Summary** (models, hypothesis, limits, link with information theory, probabilities, algebra)

## Reference

> Francis Bach. [Introduction to Probabilistic Graphical Models](https://www.di.ens.fr/~fbach/courses/fall2018/). 2018.

> Michel Deudon. [Learning semantic similarity in a continuous space](https://proceedings.neurips.cc/paper/2018/hash/97e8527feaf77a97fc38f34216141515-Abstract.html). Advances in neural information processing systems. vol 31. 2018.
